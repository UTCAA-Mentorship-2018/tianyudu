\documentclass{article}
\title{Home Credit Default Risk Analysis}
\date{Oct. 2018}
\author{Tianyu Du}

\usepackage{spikey}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{chngcntr}


\begin{document}
	\maketitle
	\paragraph{Last update} {\today}
	\tableofcontents
	\newpage
	\section{Light Gradient Boosting Machine}
		\subsection*{The Task}
			\par In this competition, we are going to preform a \emph{binary classification problem}. In our basic model, only features from \texttt{application\_train.csv} are used.
		\subsection{Data Preprocessing}
			\begin{notation}
				Each row of raw dataset represents one single \emph{observation}(sample) and each column represents one \emph{feature} or the \emph{target}.
			\end{notation}
			\subsubsection{Dropping Invalid Data}
				\paragraph{Basic} \texttt{SK\_ID\_CURR} and \texttt{TARGET} are excluded from training features.
				\paragraph{Dropping Invalid Features}
				Many of the columns in the raw dataset involves \emph{invalid observations} (marked as \texttt{nan}). 
				
				One can specify a \texttt{DROP\_THERSHOLD} parameter ranges from $0\%$ to $100\%$ (default at $10\%$). Features that are available for too few observations are considered as \emph{invalid features}. Invalid features will be excluded.
				
				With the default setting, $10\%$, features containing more than $10\%$ invalid observations will be considered as invalid features. And we have $63$ features left with the default $10\%$ threshold.
				
				\paragraph{Dropping Specific Features} One can specify a \texttt{DROP\_COLUMNS} parameter to manually drop some features from the raw dataset. In our baseline model, no feature besides \texttt{SK\_ID\_CURR} and \texttt{TARGET} are dropped through this manner.
				
				\paragraph{Dropping Invalid Observations} After feature selection, observations still with \texttt{nan} feature(s) will be dropped, so that we won't have any \texttt{nan} element in our feature and target dataset for model training.
				
				
			\subsubsection{Encoding Categorical Features}
				\paragraph{} After feature selection, many of them are \emph{categorical features} in string format. And they are encoded so that they can be fed into our model.
				\paragraph{Integer Encoder} The package \texttt{LabelEncoder} from \texttt{sklearn} package is used  for categorical feature encoding. 
				\newline
				Categorical features are encoded
			
			\subsubsection{Splitting Training, Testing and Validation Sets}
			
			\subsubsection{Scaling Data}
				\paragraph{} Generally, two methods of scaling, normalization, and standardization, are considered to handle features with significantly different ranges.
                
                After integer-encoding categorical features, all features in the training set are in floating format. We can, therefore, scale them so that all features have similar distributions.
                
                Specifically, since the testing set is unobservable for our model and validation set is used to simulate this property, we are going to fit our feature-scalers on the training set only.
                
                Then, in the model validation phase and evaluate phase, feature-scalers are applied on validation and testing sets.
				
				\paragraph{Normalization.} For each feature $j$, we fit a feature scaler from the training set. Let $\max(x_j)$ and $\min(x_j)$ denotes the largest and smallest observed values of feature $j$ in training set.
				
				For each observation $x_{ij}$, the normalized proxy $z_{ij}$ is generated following equation (1)
				\begin{equation}
					z_{ij} = \frac{x_{ij} - \min(x_j)}{\max(x_j) - \min(x_j)}
				\end{equation}
				
				Note that after normalization, ranges of all features are normalized and bounded to $[0,1]$.
				
				\paragraph{Standardization.} For each feature $j$, firstly, the training set standard deviation and mean are calculated and denoted as $\sigma_{x_j}$ and $\overline{x}_j$.
				
				Then, for each observation $x_{ij}$, the standardized proxy is generated from the equation (2)
				\begin{equation}
					z_{ij} = \frac{x_{ij} - \sigma_{x_j}}{\sigma_{x_j}}
				\end{equation}
				
				After standardization, every proxy feature has the same mean of $0$ and variance of 1.
				
				\begin{remark}
					\hl{In our model, standardization methods are used as default.}		
				\end{remark}
\end{document}



























